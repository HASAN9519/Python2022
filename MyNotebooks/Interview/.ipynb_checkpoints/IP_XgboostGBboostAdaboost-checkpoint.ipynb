{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#How-To-Learn-Machine-Learning-Algorithms-For-Interviews\" data-toc-modified-id=\"How-To-Learn-Machine-Learning-Algorithms-For-Interviews-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>How To Learn Machine Learning Algorithms For Interviews</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Tree-Classifier-And-Regressor\" data-toc-modified-id=\"Decision-Tree-Classifier-And-Regressor-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Decision Tree Classifier And Regressor</a></span></li><li><span><a href=\"#Xgboost-Classifier-And-Regressor,-GB-Algorithm,-Adaboost\" data-toc-modified-id=\"Xgboost-Classifier-And-Regressor,-GB-Algorithm,-Adaboost-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Xgboost Classifier And Regressor, GB Algorithm, Adaboost</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-What-Are-the-Basic-Assumption?\" data-toc-modified-id=\"1.-What-Are-the-Basic-Assumption?-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>1. What Are the Basic Assumption?</a></span></li></ul></li><li><span><a href=\"#Missing-Values\" data-toc-modified-id=\"Missing-Values-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Missing Values</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.-Advantages\" data-toc-modified-id=\"2.-Advantages-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>2. Advantages</a></span></li><li><span><a href=\"#3.-Disadvantages\" data-toc-modified-id=\"3.-Disadvantages-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>3. Disadvantages</a></span></li><li><span><a href=\"#4.-Whether-Feature-Scaling-is-required?\" data-toc-modified-id=\"4.-Whether-Feature-Scaling-is-required?-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>4. Whether Feature Scaling is required?</a></span></li><li><span><a href=\"#6.-Impact-of-outliers?\" data-toc-modified-id=\"6.-Impact-of-outliers?-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>6. Impact of outliers?</a></span></li><li><span><a href=\"#Types-of-Problems-it-can-solve(Supervised)\" data-toc-modified-id=\"Types-of-Problems-it-can-solve(Supervised)-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>Types of Problems it can solve(Supervised)</a></span></li><li><span><a href=\"#Performance-Metrics\" data-toc-modified-id=\"Performance-Metrics-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>Performance Metrics</a></span></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-1.3.7\"><span class=\"toc-item-num\">1.3.7&nbsp;&nbsp;</span>Classification</a></span></li><li><span><a href=\"#Regression\" data-toc-modified-id=\"Regression-1.3.8\"><span class=\"toc-item-num\">1.3.8&nbsp;&nbsp;</span>Regression</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Learn Machine Learning Algorithms For Interviews\n",
    "\n",
    "#### Decision Tree Classifier And Regressor\n",
    "Interview Questions:\n",
    "1. Decision Tree \n",
    "2. Entropy, Information Gain, Gini Impurity\n",
    "3. Decision Tree Working For Categorical and Numerical Features\n",
    "4. What are the scenarios where Decision Tree works well\n",
    "5. Decision Tree Low Bias And High Variance- Overfitting\n",
    "6. Hyperparameter Techniques\n",
    "7. Library used for constructing decision tree\n",
    "8. Impact of Outliers Of Decision Tree\n",
    "9. Impact of mising values on Decision Tree\n",
    "10. Does Decision Tree require Feature Scaling\n",
    "\n",
    "#### Xgboost Classifier And Regressor, GB Algorithm, Adaboost\n",
    "\n",
    " \n",
    "\n",
    "Decision Tree Theoretical Understanding:\n",
    "\n",
    "1. Tutorial 37:Entropy In Decision Tree https://www.youtube.com/watch?v=1IQOtJ4NI_0\n",
    "2. Tutorial 38:Information Gain https://www.youtube.com/watch?v=FuTRucXB9rA\n",
    "3. Tutorial 39:Gini Impurity https://www.youtube.com/watch?v=5aIFgrrTqOw\n",
    "4. Tutorial 40: Decision Tree For Numerical Features: https://www.youtube.com/watch?v=5O8HvA9pMew \n",
    "5. How To Visualize DT: https://www.youtube.com/watch?v=ot75kOmpYjI\n",
    "\n",
    "Theoretical Understanding:\n",
    "\n",
    "1. Ensemble technique(Bagging): https://www.youtube.com/watch?v=KIOeZ5cFZ50\n",
    "2. Adaboost(Boosting Technique):https://www.youtube.com/watch?v=NLRO1-jp5F8\n",
    "3. Gradient Boosting In Depth Intuition Part 1: https://www.youtube.com/watch?v=Nol1hVtLOSg\n",
    "4. Gradient Boosting In Depth Intuition Part 2: https://www.youtube.com/watch?v=Oo9q6YtGzvc\n",
    "5. Xgboost Classifier Indepth Intuition: https://www.youtube.com/watch?v=gPciUPwWJQQ\n",
    "6. Xgboost Regression Indpeth Intuition: https://www.youtube.com/watch?v=w-_vmVfpssg\n",
    "7. Implementation of Xgboost: https://youtu.be/9HomdnM12o4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. What Are the Basic Assumption?\n",
    "There are no such assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "1. Adaboost can handle mising values\n",
    "2. Xgboosst and GBoost cannot handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  2. Advantages\n",
    "Advantages of Adaboost\n",
    "\n",
    "1. <b>Doesn't Overfit<b>\n",
    "\n",
    "2. It has few parameters to tune\n",
    "    \n",
    "Advantages of Gradient Boost And Xgboost\n",
    "    <b>\n",
    "1. It has a great performance\n",
    "2. It can solve complex non linear functions \n",
    "3. It is better in solve any kind of ML usecases.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Disadvantages\n",
    "Disadvantages of Gradient Boosting And Xgboost\n",
    "\n",
    "1.It requires some amount of parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Whether Feature Scaling is required?\n",
    "No\n",
    "\n",
    "##### 6. Impact of outliers?\n",
    "Robust to Outliers in Gradient Boosting And Xgboost,\n",
    "Sensitive to outliers in Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types of Problems it can solve(Supervised)\n",
    "1. Classification\n",
    "2. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification\n",
    "1. Confusion Matrix \n",
    "2. Precision,Recall, F1 score\n",
    "\n",
    "##### Regression\n",
    "1. R2,Adjusted R2\n",
    "2. MSE,RMSE,MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
